version: "1.0"
name: inferenceAPI

services:
  inferenceapi:
    build:
      context: .
      dockerfile: Dockerfile
    # image: shivgupta121/mlflowpipeline
    container_name: inferenceapi
    ports:
      - 5001:5001
    restart: always
    tty: true
    command: uvicorn basic_app:api --host 0.0.0.0 --port 5001 --reload
    # volumes:
    #   - artifactStore:/home/mlruns
# volumes:
#   artifactStore:
#     external: true
# networks:
#   default:
#     name: mflowtrackingserver_default
#     external: true